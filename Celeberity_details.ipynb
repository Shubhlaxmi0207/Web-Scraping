{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Celeberity details",
      "provenance": [],
      "authorship_tag": "ABX9TyPRDP2VlsPYmS11QzjlbNxm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shubhlaxmi0207/Web-Scraping/blob/master/Celeberity_details.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgq5YzvbJ7yB",
        "colab_type": "text"
      },
      "source": [
        "Problem- Crawl popular websites & create a database of Indian movie celebrities with their images and personality traits.\n",
        "\n",
        "\n",
        "\n",
        "First of all we would see what does the web scraping actually mean.\n",
        "Basically,we need to **extract** data whatever we want from website,**process** it and store them in a **structure and format.**\n",
        "\n",
        "\n",
        "According to above problem the list of some popular websites are given below-\n",
        "\n",
        "URL1-https://www.forbesindia.com/lists/2018-celebrity-100/1735/1\n",
        "\n",
        "URL2-https://inc42.com/features/2019-in-review-bollywood-celebs-investments-in-startups-this-year/\n",
        "\n",
        "URL3-https://gulfnews.com/photos/entertainment/top-bollywood-stars-and-their-net-worth-from-shah-rukh-khan-to-aamir-khan-how-much-are-these-entertainers-worth-1.1578401333893\n",
        "\n",
        "URL4-https://www.news18.com/photogallery/sports/top-30-celebrities-on-the-2019-forbes-india-celebrity-100-list-2430425.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZSha1b0Q-Ci",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1--qSWj1Pyyt",
        "colab_type": "text"
      },
      "source": [
        "In 3 major steps we do web scraping.\n",
        "\n",
        "\n",
        "**Step1** -we should know which website are we going to deal with or to extract data from.\n",
        "\n",
        "**Step 2**-Procedure applied to extract the data.\n",
        "\n",
        "**Step 3**- Put the unorganised data into organised or frame or format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_2iBHMdQoay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step 1\n",
        "\n",
        "#From above I can choose any of the websites and extract data at a time\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjejEjXNRpK7",
        "colab_type": "text"
      },
      "source": [
        "**Step2**\n",
        "How to extract data???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30ZxB06aSBw0",
        "colab_type": "text"
      },
      "source": [
        "According to question the data we need are-\n",
        "\n",
        "\n",
        "1-Celebrity Name\n",
        "\n",
        "2-Celebrity Image or image URL\n",
        "\n",
        "3-celebrity personality trait"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTxm-rK0R9If",
        "colab_type": "text"
      },
      "source": [
        "Let's set up our system i.e. some neccesery steps before performing these steps\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS12r_6UTNqs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7ea5cb72-5737-497e-fdc0-51749da2fb7e"
      },
      "source": [
        "#installing Scrapy\n",
        "#About Scrapy!!!,Scrapy is a framework for large scale web scraping.It h=gives us tools to extract. process and store data in a format we want, efficiently.\n",
        "pip install Scrapy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Scrapy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/d3/5af102af577f57f706fcb302ea47d40e09355778488de904b3594d4e48d2/Scrapy-2.1.0-py2.py3-none-any.whl (239kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 17.9MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████                            | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy) (4.2.6)\n",
            "Collecting w3lib>=1.17.0\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/45/1ba17c50a0bb16bd950c9c2b92ec60d40c8ebda9f3371ae4230c437120b6/w3lib-1.21.0-py2.py3-none-any.whl\n",
            "Collecting PyDispatcher>=2.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n",
            "Collecting queuelib>=1.4.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n",
            "Collecting pyOpenSSL>=16.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/de/f8342b68fa9e981d348039954657bdf681b2ab93de27443be51865ffa310/pyOpenSSL-19.1.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.3MB/s \n",
            "\u001b[?25hCollecting parsel>=1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/86/c8/fc5a2f9376066905dfcca334da2a25842aedfda142c0424722e7c497798b/parsel-1.5.2-py2.py3-none-any.whl\n",
            "Collecting service-identity>=16.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618d43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-any.whl\n",
            "Collecting protego>=0.1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/6e/bf6d5e4d7cf233b785719aaec2c38f027b9c2ed980a0015ec1a1cced4893/Protego-0.1.16.tar.gz (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 47.1MB/s \n",
            "\u001b[?25hCollecting Twisted>=17.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/04/1a664c9e5ec0224a1c1a154ddecaa4dc7b8967521bba225efcc41a03d5f3/Twisted-20.3.0-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 35.3MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9.1\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Collecting zope.interface>=4.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/33/565274c28a11af60b7cfc0519d46bde4125fcd7d32ebc0a81b480d0e8da6/zope.interface-5.1.0-cp36-cp36m-manylinux2010_x86_64.whl (234kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 53.3MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/04/686efee2dcdd25aecf357992e7d9362f443eb182ecd623f882bc9f7a6bba/cryptography-2.9.2-cp35-abi3-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 25.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from w3lib>=1.17.0->Scrapy) (1.12.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->Scrapy) (0.2.8)\n",
            "Requirement already satisfied: attrs>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->Scrapy) (19.3.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->Scrapy) (0.4.8)\n",
            "Collecting incremental>=16.10.1\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n",
            "Collecting PyHamcrest!=1.10.0,>=1.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/16/e54cc65891f01cb62893540f44ffd3e8dab0a22443e1b438f1a9f5574bee/PyHamcrest-2.0.2-py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.7MB/s \n",
            "\u001b[?25hCollecting Automat>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/dd/83/5f6f3c1a562674d65efc320257bdc0873ec53147835aeef7762fe7585273/Automat-20.2.0-py2.py3-none-any.whl\n",
            "Collecting hyperlink>=17.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/91/e916ca10a2de1cb7101a9b24da546fb90ee14629e23160086cf3361c4fb8/hyperlink-19.0.0-py2.py3-none-any.whl\n",
            "Collecting constantly>=15.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.1.3->Scrapy) (46.1.3)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0->Scrapy) (1.14.0)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.6/dist-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->Scrapy) (2.8)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->Scrapy) (2.20)\n",
            "Building wheels for collected packages: PyDispatcher, protego\n",
            "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-cp36-none-any.whl size=11515 sha256=8b97a38a4138ad95c0ac799c9dbf8bfb2a740986a3c6c631ac839c368c4117f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/99/96/cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n",
            "  Building wheel for protego (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for protego: filename=Protego-0.1.16-cp36-none-any.whl size=7765 sha256=80ef101cfc9c5bf4965d8dbb44b3cdae41c80dd5c1ed78e0b410819a9f832474\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/01/d1/4a2286a976dccd025ba679acacfe37320540df0f2283ecab12\n",
            "Successfully built PyDispatcher protego\n",
            "Installing collected packages: w3lib, PyDispatcher, queuelib, cryptography, pyOpenSSL, cssselect, parsel, service-identity, protego, incremental, PyHamcrest, Automat, hyperlink, constantly, zope.interface, Twisted, Scrapy\n",
            "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 PyHamcrest-2.0.2 Scrapy-2.1.0 Twisted-20.3.0 constantly-15.1.0 cryptography-2.9.2 cssselect-1.1.0 hyperlink-19.0.0 incremental-17.5.0 parsel-1.5.2 protego-0.1.16 pyOpenSSL-19.1.0 queuelib-1.5.0 service-identity-18.1.0 w3lib-1.21.0 zope.interface-5.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzkZNm4CUY-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fetch the website in order to fetch the html or css content behind the webside.this is bascally to fetch the name or image\n",
        "#for examle I am using URL1\n",
        "URL1=https://www.forbesindia.com/lists/2018-celebrity-100/1735/1\n",
        "fetch(URL1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf7OcLq4WbQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fetch returns object response\n",
        "#to  view the contains the downloaded information of website\n",
        "view(response)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7fB354mY8n5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to see the raw content. This can be the source code which contains the html,css script\n",
        "print response.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcmcO40Mf8n8",
        "colab_type": "text"
      },
      "source": [
        "#In the question we have to extract name of the celebrity which in URL given in table format\n",
        "so the format of creating table  with html\n",
        "<table style=\"width:100%\">\n",
        "  <tr>\n",
        "    <th>Rank</th>\n",
        "    <th>Name</th>\n",
        "    <th>Earnings(In Rs.crore)</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Salman khan<img src=\"https://www.forbesindia.com/media/list/Salman_181.jpg\"></td>\n",
        "    <td>253.25</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2</td>\n",
        "    <td>Virat Kohli<img src=\"https://www.forbesindia.com/media/list/Virat_181.jpg\"></td>\n",
        "    <td>228.09</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccm4k_j2jNU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#table style=\"width:100%\">\n",
        "#so if we name and image link from the below code we would be able to prepare informative table\n",
        "  <tr>\n",
        "    <th>Rank</th>\n",
        "    <th>Name</th>\n",
        "    <th>Earnings(In Rs.crore)</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Salman khan<img src=\"https://www.forbesindia.com/media/list/Salman_181.jpg\"></td>\n",
        "    <td>253.25</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2</td>\n",
        "    <td>Virat Kohli<img src=\"https://www.forbesindia.com/media/list/Virat_181.jpg\"></td>\n",
        "    <td>228.09</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXJDqBkcl2Ki",
        "colab_type": "text"
      },
      "source": [
        "**WRITING CUSTOM SPIDERS**\n",
        "\n",
        "Since scraping a large project is not easy task.Hence, crawling which is to be done in whole website should store the data in a class objects.\n",
        "\n",
        "The class mainly contains the details about the website such as domains allowed to crawl,starting URL etc. and a member funtion can be called - parse(this function called whenever the crawler successfully crawls a URL.\n",
        "\n",
        "\n",
        "**FORMAT**\n",
        "\n",
        "\n",
        "       \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZKHO05g4XgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CELEBBOT(scrapy,Spider):\n",
        "\n",
        "      name='celebbot'\n",
        "      allow=['www.forbesindia.com/']\n",
        "      startURL=['https://www.forbesindia.com/lists/2018-celebrity-100/1735/1']\n",
        "\n",
        "def parse(self,response):\n",
        "  \n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xLe5mSOqhej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#the parse function contains following code which is to be defined above...\n",
        "def parse(self, response):\n",
        "        #Extracting the content\n",
        "        code...for extraction\n",
        "       \n",
        "        #Give the extracted content row wise\n",
        "        for item in zip(Name,Image URL,Earnings(In Rs.crores)):\n",
        "            #create a dictionary to store the scraped info\n",
        "            scraped_info = {\n",
        "                'Name' : item[0],\n",
        "                'Image URL' : item[1],\n",
        "                'Earnings(In Rs.crores)' : item[2],\n",
        "                \n",
        "            }\n",
        "\n",
        "            #yield or give the scraped info to scrapy\n",
        "            yield scraped_info"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF-RtjJfrjFe",
        "colab_type": "text"
      },
      "source": [
        "We saw above fuction stores all the information in a dictionary called scraped_info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuTbCKcGt7fW",
        "colab_type": "text"
      },
      "source": [
        "Now let us run the spider\n",
        "using following command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzQKlZp1t9qU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scrapy crawl celebbot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_fRYhIXuGF5",
        "colab_type": "text"
      },
      "source": [
        "Which will return some data stuff\n",
        "\n",
        "\n",
        "**Step3**-\n",
        "\n",
        "Export the data into CSV format\n",
        "Scrapy provides this functionality where we can export the downloaded content in various formats\n",
        "\n",
        "Open the settings.py file and add the following code to it:\n",
        "FEED_FORMAT = \"csv\"\n"

        "FEED_URI = \"reddit.csv\"\n"

        "[FEED_FORMAT : The format in which you want the data to be exported. Supported formats are: JSON, JSON lines, XML and CSV.\n",

        "FEED_URI : The location of the exported file.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63n2WD5iuaMq",
        "colab_type": "text"
      },
      "source": [
        "Run the following code or spider"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu6qwr6zufAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scrapy crawl clebbot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krP9ZWHcvs-t",
        "colab_type": "text"
      },
      "source": [
        "Our exported csv file is now ready\n",
        "which would contain the whole data about celebrities,which was needed as the final result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yso21_3LwvFH",
        "colab_type": "text"
      },
      "source": [
        "<table style=\"width:100%\">\n",
        "  <tr>\n",
        "    <th></th>\n",
        "    <th>Name</th>\n",
        "    <th>Image URL</th>\n",
        "    <th>Earnings(In Rs.crores)</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Salman Khan</td>\n",
        "    <td>https://www.forbesindia.com/media/list/Salman_181.jpg</td>\n",
        "    <td>253.25</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2</td>\n",
        "    <td>Virat Kohali</td>\n",
        "    <td>https://www.forbesindia.com/media/list/Virat_181.jpg</td>\n",
        "    <td>228.09</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3</td>\n",
        "    <td>Akshay Kumarn</td>\n",
        "    <td>https://www.forbesindia.com/media/list/AK_181.jpg</td>\n",
        "    <td>185</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4</td>\n",
        "    <td>Deepika Padukone</td>\n",
        "    <td>https://www.forbesindia.com/media/list/Deepika_181.jpg</td>\n",
        "    <td>112.8</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>5</td>\n",
        "    <td>Mahendra Singh Dhoni</td>\n",
        "    <td>https://www.forbesindia.com/media/list/Dhoni_181.jpg</td>\n",
        "    <td>101.77</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d71ZrTqR3BbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
